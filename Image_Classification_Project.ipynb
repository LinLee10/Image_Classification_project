{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\").values\n",
    "X = df[:, 1:]\n",
    "y = df[:, 0]\n",
    "X = X / 255.0 #normalize values, from 255 to 1, just works faster and more accurate\n",
    "# prevents larger pixels from overpowering smaller ones\n",
    "\n",
    "X = X.astype(\"float32\")\n",
    "# need float point values for smooth propagation during training\n",
    "\n",
    "image = X[0].reshape(28, 28)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (33600, 784) (33600,)\n",
      "Validation set: (4200, 784) (4200,)\n",
      "Test set: (4200, 784) (4200,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "# split into train 80%, validation 10% and test 10% \n",
    "# validation set to tune model hyperparameters, prevents overfitting \n",
    "\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8730 - loss: 0.4105 - val_accuracy: 0.9776 - val_loss: 0.0738\n",
      "Epoch 2/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9825 - loss: 0.0562 - val_accuracy: 0.9874 - val_loss: 0.0451\n",
      "Epoch 3/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9894 - loss: 0.0347 - val_accuracy: 0.9864 - val_loss: 0.0502\n",
      "Epoch 4/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9921 - loss: 0.0240 - val_accuracy: 0.9902 - val_loss: 0.0348\n",
      "Epoch 5/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9947 - loss: 0.0176 - val_accuracy: 0.9893 - val_loss: 0.0350\n",
      "Epoch 6/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9951 - loss: 0.0132 - val_accuracy: 0.9910 - val_loss: 0.0371\n",
      "Epoch 7/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9961 - loss: 0.0108 - val_accuracy: 0.9871 - val_loss: 0.0484\n",
      "Epoch 8/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9969 - loss: 0.0088 - val_accuracy: 0.9898 - val_loss: 0.0362\n",
      "Epoch 9/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9978 - loss: 0.0060 - val_accuracy: 0.9900 - val_loss: 0.0365\n",
      "Epoch 10/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9973 - loss: 0.0080 - val_accuracy: 0.9895 - val_loss: 0.0378\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0451\n",
      "Test Accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "# why use a neural network here: learns patterns in terms of curves and straight lines, and \n",
    "# recognizes numbers even if theyre tilted\n",
    "\n",
    "#then change images from (num_samples, 784) to (num_samples, 28, 28, 1) because CNNs use 2D input, not 1D\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_val = X_val.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),#detect image features\n",
    "    layers.MaxPooling2D(pool_size=(2,2)), #reduce image sizing for speed\n",
    "\n",
    "    layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'), \n",
    "    layers.MaxPooling2D(pool_size=(2,2)),  #size down more \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  \n",
    "])\n",
    "\n",
    "#define optimizer, loss function, and metrics used, just ways to measure model stuff \n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "#an epoch is one complete pass through the dataset during training, \n",
    "#train the mf model, use the model to run over the training data 10 times (all the data)\n",
    "\n",
    "training = model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 10, batch_size = 32)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0451\n",
      "Test Accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(f\"Predicted: {predicted_labels[i]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.9031 - loss: 0.3251 - val_accuracy: 0.9848 - val_loss: 0.0459\n",
      "Epoch 2/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9789 - loss: 0.0664 - val_accuracy: 0.9764 - val_loss: 0.0812\n",
      "Epoch 3/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9818 - loss: 0.0528 - val_accuracy: 0.9890 - val_loss: 0.0344\n",
      "Epoch 4/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.9876 - loss: 0.0416 - val_accuracy: 0.9912 - val_loss: 0.0306\n",
      "Epoch 5/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.9894 - loss: 0.0350 - val_accuracy: 0.9902 - val_loss: 0.0324\n",
      "Epoch 6/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9908 - loss: 0.0314 - val_accuracy: 0.9881 - val_loss: 0.0367\n",
      "Epoch 7/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9903 - loss: 0.0312 - val_accuracy: 0.9910 - val_loss: 0.0293\n",
      "Epoch 8/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9917 - loss: 0.0284 - val_accuracy: 0.9898 - val_loss: 0.0345\n",
      "Epoch 9/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9926 - loss: 0.0240 - val_accuracy: 0.9864 - val_loss: 0.0453\n",
      "Epoch 10/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9935 - loss: 0.0197 - val_accuracy: 0.9905 - val_loss: 0.0351\n",
      "Epoch 11/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9940 - loss: 0.0199 - val_accuracy: 0.9917 - val_loss: 0.0327\n",
      "Epoch 12/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9951 - loss: 0.0158 - val_accuracy: 0.9902 - val_loss: 0.0359\n",
      "Epoch 13/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9958 - loss: 0.0133 - val_accuracy: 0.9924 - val_loss: 0.0328\n",
      "Epoch 14/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9947 - loss: 0.0150 - val_accuracy: 0.9905 - val_loss: 0.0337\n",
      "Epoch 15/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9956 - loss: 0.0130 - val_accuracy: 0.9933 - val_loss: 0.0241\n"
     ]
    }
   ],
   "source": [
    "#Slight different model with inclusion of \n",
    "# dropout (randomly turn off 50% of neurons) to prevent overfitting\n",
    "# batch normalization - normalized data\n",
    "# more layers, more complex features\n",
    "# uses dropout, batch normalization\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.BatchNormalization(), #normalization\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.BatchNormalization(), #normalization\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5), #dropout\n",
    "    layers.Dense(10, activation='softmax')  \n",
    "])\n",
    "\n",
    "#retrain\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit( X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=32 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linlee10/Desktop/Personal_projects/venv/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.9634 - loss: 0.1318 - val_accuracy: 0.9931 - val_loss: 0.0203\n",
      "Epoch 2/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9808 - loss: 0.0675 - val_accuracy: 0.9907 - val_loss: 0.0267\n",
      "Epoch 3/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9855 - loss: 0.0519 - val_accuracy: 0.9921 - val_loss: 0.0236\n",
      "Epoch 4/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9854 - loss: 0.0492 - val_accuracy: 0.9917 - val_loss: 0.0260\n",
      "Epoch 5/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9859 - loss: 0.0457 - val_accuracy: 0.9931 - val_loss: 0.0204\n",
      "Epoch 6/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9887 - loss: 0.0387 - val_accuracy: 0.9919 - val_loss: 0.0216\n",
      "Epoch 7/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9872 - loss: 0.0385 - val_accuracy: 0.9919 - val_loss: 0.0277\n",
      "Epoch 8/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9897 - loss: 0.0347 - val_accuracy: 0.9936 - val_loss: 0.0185\n",
      "Epoch 9/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.9889 - loss: 0.0341 - val_accuracy: 0.9931 - val_loss: 0.0195\n",
      "Epoch 10/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.9891 - loss: 0.0348 - val_accuracy: 0.9929 - val_loss: 0.0192\n",
      "Epoch 11/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9901 - loss: 0.0317 - val_accuracy: 0.9936 - val_loss: 0.0187\n",
      "Epoch 12/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.9908 - loss: 0.0281 - val_accuracy: 0.9919 - val_loss: 0.0248\n",
      "Epoch 13/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9908 - loss: 0.0295 - val_accuracy: 0.9943 - val_loss: 0.0166\n",
      "Epoch 14/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9907 - loss: 0.0322 - val_accuracy: 0.9919 - val_loss: 0.0230\n",
      "Epoch 15/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.9912 - loss: 0.0266 - val_accuracy: 0.9931 - val_loss: 0.0199\n"
     ]
    }
   ],
   "source": [
    "#data augmentation - artificially increase dataset size via transformations to the images\n",
    "# makes CNN generalize better by creating more variations\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  # Rotate images up to 10 degrees\n",
    "    zoom_range=0.1,  # Zoom in/out by 10%\n",
    "    width_shift_range=0.1,  # Shift image left/right by 10%\n",
    "    height_shift_range=0.1  # Shift image up/down by 10%\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "#train\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),  # Use the augmented images\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch size: 32\n",
      "Epoch 1/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9919 - loss: 0.0287 - val_accuracy: 0.9936 - val_loss: 0.0184\n",
      "Epoch 2/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9906 - loss: 0.0310 - val_accuracy: 0.9940 - val_loss: 0.0194\n",
      "Epoch 3/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9923 - loss: 0.0258 - val_accuracy: 0.9957 - val_loss: 0.0167\n",
      "Epoch 4/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9909 - loss: 0.0289 - val_accuracy: 0.9931 - val_loss: 0.0180\n",
      "Epoch 5/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.9924 - loss: 0.0248 - val_accuracy: 0.9936 - val_loss: 0.0209\n",
      "Epoch 6/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.9913 - loss: 0.0268 - val_accuracy: 0.9945 - val_loss: 0.0161\n",
      "Epoch 7/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - accuracy: 0.9924 - loss: 0.0243 - val_accuracy: 0.9940 - val_loss: 0.0215\n",
      "Epoch 8/10\n",
      "\u001b[1m 826/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.9934 - loss: 0.0210"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m]:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining with batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Personal_projects/venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#tuning hyperparameters (learning rate, batch size, filters, dropout rate)\n",
    "# hyperparameters - settings that control how the model learns, \n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)  # Start with 0.001\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "for batch_size in [32, 64, 128]:\n",
    "    print(f\"\\nTraining with batch size: {batch_size}\")\n",
    "    history = model.fit(\n",
    "        datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=10\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define improved CNN with optimized hyperparameters\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.4),  # Optimized dropout\n",
    "\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(10, activation='softmax')  \n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=64),validation_data=(X_val, y_val),\n",
    "                     epochs=15,callbacks=[lr_scheduler]  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
