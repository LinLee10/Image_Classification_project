{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcZXX4aqVsgl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iDpeMDUVti_",
        "outputId": "97cf094c-7e1c-4229-e4b3-ef4346ae8e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading dataset...\")\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "#load 30,000 images in\n",
        "X_train = X_train[:30000]\n",
        "y_train = y_train[:30000]\n",
        "\n",
        "#normalize values\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_train = X_train.astype(\"float32\")\n",
        "X_test = X_test.astype(\"float32\")\n",
        "#reshape\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3IoFv9UyVweE",
        "outputId": "34727d70-174b-47e1-af56-58439857a8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set: (24000, 28, 28, 1) (24000,)\n",
            "Validation set: (6000, 28, 28, 1) (6000,)\n",
            "Test set: (10000, 28, 28, 1) (10000,)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set:\", X_train.shape, y_train.shape)\n",
        "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
        "print(\"Test set:\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAhXRJVcVwyj",
        "outputId": "6dbf51b2-7eae-45c4-d319-4337ea054788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building baseline CNN model...\n",
            "Training baseline model...\n",
            "Epoch 1/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.8404 - loss: 0.4897 - val_accuracy: 0.9740 - val_loss: 0.0914\n",
            "Epoch 2/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9797 - loss: 0.0653 - val_accuracy: 0.9793 - val_loss: 0.0714\n",
            "Epoch 3/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9872 - loss: 0.0373 - val_accuracy: 0.9850 - val_loss: 0.0596\n",
            "Epoch 4/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.9903 - loss: 0.0303 - val_accuracy: 0.9820 - val_loss: 0.0658\n",
            "Epoch 5/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9936 - loss: 0.0189 - val_accuracy: 0.9857 - val_loss: 0.0584\n",
            "Epoch 6/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.9958 - loss: 0.0141 - val_accuracy: 0.9860 - val_loss: 0.0564\n",
            "Epoch 7/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.9965 - loss: 0.0114 - val_accuracy: 0.9863 - val_loss: 0.0545\n",
            "Epoch 8/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.9965 - loss: 0.0088 - val_accuracy: 0.9865 - val_loss: 0.0610\n",
            "Epoch 9/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9977 - loss: 0.0073 - val_accuracy: 0.9855 - val_loss: 0.0652\n",
            "Epoch 10/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9962 - loss: 0.0106 - val_accuracy: 0.9882 - val_loss: 0.0554\n",
            "Evaluating model...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9868 - loss: 0.0538\n",
            "Test Accuracy: 0.9899\n"
          ]
        }
      ],
      "source": [
        "print(\"Building baseline CNN model...\")\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model\n",
        "print(\"Training baseline model...\")\n",
        "training = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Evaluating model...\")\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O_s3SQHVyQf",
        "outputId": "40fc6427-f7ef-4a2b-c395-3f73f0b8163f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying data augmentation...\n"
          ]
        }
      ],
      "source": [
        "# Data augmentation\n",
        "print(\"Applying data augmentation...\")\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "datagen.fit(X_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49bZX91RVzTd",
        "outputId": "c1aa6981-e218-4469-b66d-c9987bd8f354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building improved CNN model...\n",
            "Training improved model...\n",
            "Epoch 1/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 540ms/step - accuracy: 0.7276 - loss: 0.9277 - val_accuracy: 0.7527 - val_loss: 0.7924 - learning_rate: 0.0010\n",
            "Epoch 2/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 499ms/step - accuracy: 0.9482 - loss: 0.1670 - val_accuracy: 0.9843 - val_loss: 0.0499 - learning_rate: 0.0010\n",
            "Epoch 3/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 494ms/step - accuracy: 0.9609 - loss: 0.1189 - val_accuracy: 0.9880 - val_loss: 0.0389 - learning_rate: 0.0010\n",
            "Epoch 4/15\n",
            "\u001b[1m275/469\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 477ms/step - accuracy: 0.9682 - loss: 0.1042"
          ]
        }
      ],
      "source": [
        "# Improved CNN model with dropout and batch normalization\n",
        "print(\"Building improved CNN model...\")\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"Training improved model...\")\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=64), validation_data=(X_val, y_val), epochs=15, callbacks=[lr_scheduler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0GlhC5cebWP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}